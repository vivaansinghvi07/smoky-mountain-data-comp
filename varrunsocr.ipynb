{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers \n",
    "import matplotlib.pyplot as plt\n",
    "import cirq\n",
    "import sympy\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image as tfimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit\n",
    "import tensorflow_datasets \n",
    "from keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 0 classes.\n",
      "Using 0 files for training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory dataset/. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[39m=\u001b[39m image_dataset_from_directory(\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdataset/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     labels\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minferred\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m     label_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mint\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     image_size\u001b[39m=\u001b[39;49m(\u001b[39m256\u001b[39;49m, \u001b[39m128\u001b[39;49m),\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39m123\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     subset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/smokey-mountain-comp/.venv/lib/python3.10/site-packages/keras/utils/image_dataset.py:297\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m image_paths, labels \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39mget_training_or_validation_split(\n\u001b[1;32m    294\u001b[0m     image_paths, labels, validation_split, subset\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m image_paths:\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo images found in directory \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAllowed formats: \u001b[39m\u001b[39m{\u001b[39;00mALLOWLIST_FORMATS\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m dataset \u001b[39m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[1;32m    303\u001b[0m     image_paths\u001b[39m=\u001b[39mimage_paths,\n\u001b[1;32m    304\u001b[0m     image_size\u001b[39m=\u001b[39mimage_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m     crop_to_aspect_ratio\u001b[39m=\u001b[39mcrop_to_aspect_ratio,\n\u001b[1;32m    311\u001b[0m )\n\u001b[1;32m    312\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mprefetch(tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n",
      "\u001b[0;31mValueError\u001b[0m: No images found in directory dataset/. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    directory='synthetic-data/images',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 128),\n",
    "    shuffle=True,\n",
    "    seed=123, \n",
    "    validation_split=0.1,\n",
    "    subset=\"training\")\n",
    "\n",
    "# validation_ds = image_dataset_from_directory(\n",
    "#     directory='validation_data/',\n",
    "#     labels='inferred',\n",
    "#     label_mode='int',\n",
    "#     batch_size=32,\n",
    "#     image_size=(256, 128),\n",
    "#     shffle=True,\n",
    "#     seed=123, \n",
    "#     validation_split=0.1,\n",
    "#     subset=\"validation\")\n",
    "\n",
    "# x, y = [], []\n",
    "# for filename in os.listdir('synthetic-data/images'):\n",
    "#     image = cv2.imread('synthetic-data/images/' + filename)\n",
    "#     text = filename[:-4]\n",
    "#     y.append(text)\n",
    "#     x.append(image)\n",
    "    \n",
    "\n",
    "# def process_image(img):\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     blur = cv2.medianBlur(gray, 5)\n",
    "#     blur = tfimage.img_to_array(blur, dtype='uint8')\n",
    "#     thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 2)\n",
    "#     return thresh\n",
    "\n",
    "# x = map(process_image, x)\n",
    "font_ds = train_ds\n",
    "\n",
    "x_train, y_train, x_test, y_test = tfds.load(font_ds, split= 'train[:75%]')\n",
    "\n",
    "\n",
    "\n",
    "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
    "# x_train, x_test = x_train[..., np.newaxi]/255.0, x_test[..., np.newaxis]/255.0\n",
    "\n",
    "\n",
    "print(\"Number of original training examples:\", len(x_train))\n",
    "print(\"Number of original test examples:\", len(x_test))\n",
    "\n",
    "\n",
    "def filter_36(x, y):\n",
    "    keep = (y == 3) | (y == 6)\n",
    "    x, y = x[keep], y[keep]\n",
    "    y = y == 3\n",
    "    return x,y\n",
    "\n",
    "\n",
    "x_train, y_train = filter_36(x_train, y_train)\n",
    "x_test, y_test = filter_36(x_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Number of filtered training examples:\", len(x_train))\n",
    "print(\"Number of filtered test examples:\", len(x_test))\n",
    "\n",
    "\n",
    "print(y_train[0])\n",
    "\n",
    "\n",
    "plt.imshow(x_train[0, :, :, 0])\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "x_train_small = tf.image.resize(x_train, (4,4)).numpy()\n",
    "x_test_small = tf.image.resize(x_test, (4,4)).numpy()\n",
    "\n",
    "\n",
    "print(y_train[0])\n",
    "\n",
    "\n",
    "plt.imshow(x_train_small[0,:,:,0], vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "def remove_contradicting(xs, ys):\n",
    "    mapping = collections.defaultdict(set)\n",
    "    orig_x = {}\n",
    "    # Determine the set of labels for each unique image:\n",
    "    for x,y in zip(xs,ys):\n",
    "       orig_x[tuple(x.flatten())] = x\n",
    "       mapping[tuple(x.flatten())].add(y)\n",
    "\n",
    "\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    for flatten_x in mapping:\n",
    "      x = orig_x[flatten_x]\n",
    "      labels = mapping[flatten_x]\n",
    "      if len(labels) == 1:\n",
    "          new_x.append(x)\n",
    "          new_y.append(next(iter(labels)))\n",
    "      else:\n",
    "          # Throw out images that match more than one label.\n",
    "          pass\n",
    "\n",
    "\n",
    "    num_uniq_3 = sum(1 for value in mapping.values() if len(value) == 1 and True in value)\n",
    "    num_uniq_6 = sum(1 for value in mapping.values() if len(value) == 1 and False in value)\n",
    "    num_uniq_both = sum(1 for value in mapping.values() if len(value) == 2)\n",
    "\n",
    "\n",
    "    print(\"Number of unique images:\", len(mapping.values()))\n",
    "    print(\"Number of unique 3s: \", num_uniq_3)\n",
    "    print(\"Number of unique 6s: \", num_uniq_6)\n",
    "    print(\"Number of unique contradicting labels (both 3 and 6): \", num_uniq_both)\n",
    "    print()\n",
    "    print(\"Initial number of images: \", len(xs))\n",
    "    print(\"Remaining non-contradicting unique images: \", len(new_x))\n",
    "\n",
    "\n",
    "    return np.array(new_x), np.array(new_y)\n",
    "\n",
    "\n",
    "x_train_nocon, y_train_nocon = remove_contradicting(x_train_small, y_train)\n",
    "\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "x_train_bin = np.array(x_train_nocon > THRESHOLD, dtype=np.float32)\n",
    "x_test_bin = np.array(x_test_small > THRESHOLD, dtype=np.float32)\n",
    "\n",
    "\n",
    "_ = remove_contradicting(x_train_bin, y_train_nocon)\n",
    "\n",
    "\n",
    "def create_classical_model():\n",
    "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, [3, 3], activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, [3, 3], activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_classical_model()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "cnn_results = model.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "def create_fair_classical_model():\n",
    "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(4,4,1)))\n",
    "    model.add(tf.keras.layers.Dense(2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_fair_classical_model()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.fit(x_train_bin,\n",
    "          y_train_nocon,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test_bin, y_test))\n",
    "\n",
    "\n",
    "fair_nn_results = model.evaluate(x_test_bin, y_test)\n",
    "\n",
    "\n",
    "cnn_accuracy = cnn_results[1]\n",
    "fair_nn_accuracy = fair_nn_results[1]\n",
    "\n",
    "\n",
    "sns.barplot(x=[\"Classical, full\", \"Classical, fair\"],\n",
    "            y=[cnn_accuracy, fair_nn_accuracy])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
